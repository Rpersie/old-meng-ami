{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../dnn\")\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from dnn_md import DNNMultidecoder\n",
    "from hao_data import read_next_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using experiment ENC__1024_LATENT_512_DEC__1024_ACT_SELU/OPT_Adam_LR_0.0001_EPOCHS_100_BATCH_256_DEBUG_false\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables for the model we want to examine\n",
    "# NOT necessarily the current environment variables!!\n",
    "\n",
    "feat_dim=80\n",
    "left_splice=5\n",
    "right_splice=5\n",
    "\n",
    "optimizer=\"Adam\"\n",
    "learning_rate=0.0001\n",
    "epochs=100\n",
    "batch_size=256\n",
    "\n",
    "enc_layers=[1024]\n",
    "latent_dim=512\n",
    "dec_layers=[1024]\n",
    "activation=\"SELU\"\n",
    "\n",
    "enc_layers_delim=\"_\" + \"_\".join(map(str, enc_layers))\n",
    "if len(enc_layers) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_layers_delim=\"_\"\n",
    "dec_layers_delim=\"_\" + \"_\".join(map(str, dec_layers))\n",
    "if len(dec_layers) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_layers_delim=\"_\"\n",
    "\n",
    "debug_model = False\n",
    "debug_str = \"true\" if debug_model else \"false\"\n",
    "expt_name = \"ENC_%s_LATENT_%s_DEC_%s_ACT_%s/OPT_%s_LR_%s_EPOCHS_%d_BATCH_%d_DEBUG_%s\" % (enc_layers_delim, latent_dim, dec_layers_delim, activation, optimizer, str(learning_rate), epochs, batch_size, debug_str)\n",
    "\n",
    "input_dim = (left_splice + right_splice + 1) * feat_dim\n",
    "print(\"Using experiment %s\" % expt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using noreg scp /data/sls/scratch/atitus5/timit-latent-regularization/latent/ae_noreg/ENC__2048_LATENT_512_DEC__2048_ACT_SELU/OPT_Adam_LR_0.0001_EPOCHS_250_BATCH_256/latent.scp\n",
      "Using l2 scp /data/sls/scratch/atitus5/timit-latent-regularization/latent/ae_l2/ENC__2048_LATENT_512_DEC__2048_ACT_SELU/OPT_Adam_LR_0.0001_EPOCHS_250_BATCH_256/L2_LAMBDA_0.5/latent.scp\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Load per-utterance latent var values\n",
    "model_dir = \"/data/sls/scratch/atitus5/meng/models\"\n",
    "\n",
    "latent_scp_path = \"%s/ae_noreg/%s/latent.scp\" % (model_dir, expt_name_noreg)\n",
    "print(\"Using scp %s\" % latent_scp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 192 noreg utterances\n",
      "Read 192 l2 utterances\n"
     ]
    }
   ],
   "source": [
    "utt_ids = []\n",
    "\n",
    "latent_vals_noreg = []\n",
    "with open(latent_scp_path_noreg, 'r') as latent_scp:\n",
    "    for scp_line in latent_scp:\n",
    "        utt_id, latent, ark = read_next_utt(scp_line)\n",
    "        latent_vals_noreg.append(latent)\n",
    "        utt_ids.append(utt_id)\n",
    "print(\"Read %d noreg utterances\" % len(latent_vals_noreg))\n",
    "\n",
    "latent_vals_l2 = []\n",
    "with open(latent_scp_path_l2, 'r') as latent_scp:\n",
    "    utt_id_idx = 0\n",
    "    for scp_line in latent_scp:\n",
    "        utt_id, latent, ark = read_next_utt(scp_line)\n",
    "        latent_vals_l2.append(latent)\n",
    "        assert(utt_id == utt_ids[utt_id_idx])\n",
    "        utt_id_idx += 1\n",
    "print(\"Read %d l2 utterances\" % len(latent_vals_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5081335eabb4809b3119db70863d0e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.latentHeatmapComp>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def latentHeatmapComp(utt_idx):\n",
    "    x_skip = int(latent_vals_noreg[utt_idx].shape[0] / 10.0)\n",
    "    y_skip = int(latent_vals_noreg[utt_idx].shape[1] / 10.0)\n",
    "    \n",
    "    ax = sns.heatmap(np.transpose(latent_vals_noreg[utt_idx]), cmap=\"coolwarm\", xticklabels=x_skip, yticklabels=y_skip)\n",
    "    ax.set_title(\"Latent vars (noreg)\")\n",
    "    plt.xlabel(\"Frame index\")\n",
    "    plt.ylabel(\"Latent variable index\")\n",
    "    plt.savefig(\"latent_ae_noreg.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    ax = sns.heatmap(np.transpose(latent_vals_l2[utt_idx]), cmap=\"coolwarm\", xticklabels=x_skip, yticklabels=y_skip)\n",
    "    ax.set_title(\"Latent vars (l2)\")\n",
    "    plt.xlabel(\"Frame index\")\n",
    "    plt.ylabel(\"Latent variable index\")\n",
    "    plt.savefig(\"latent_ae_l2.png\")\n",
    "    plt.show()\n",
    "\n",
    "interact(latentHeatmapComp, utt_idx=(0, len(utt_ids) - 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
