Setting up environment...
Environment set up.
Training convolutional multidecoder...
Constructing model...
Done constructing model.
CNNMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxpool2d_0): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (3840 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 1024)
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 3840)
  )
  (decoder_deconv_ihm): Sequential (
    (conv2d_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 3840)
  )
  (decoder_deconv_sdm1): Sequential (
    (conv2d_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 0.509605
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.423770
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.366628
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.326322
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.290256
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.260877
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.235921
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.218596
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.213122
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.223096
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.268625
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.301300
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.334275
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.360719
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.383556
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.405371
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.395875
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.384664
====> Epoch 1: Average train loss 0.372876
====> Dev set loss: 0.051630
New best dev set loss: 0.051630
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 0.535076
Train epoch 2: [2000/18806 (10.6%)]	Loss: 0.467888
Train epoch 2: [3000/18806 (16.0%)]	Loss: 0.404719
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.361810
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.319588
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.286913
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.257775
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.237641
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.229573
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.233046
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.267430
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.291149
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.316651
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.339024
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.355841
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.373296
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.365130
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.355078
====> Epoch 2: Average train loss 0.344188
====> Dev set loss: 0.057826
No improvement in 1 epochs (best dev set loss: 0.051630)
Not saving checkpoint; no improvement made
Train epoch 3: [1000/18806 (5.3%)]	Loss: 0.526489
Train epoch 3: [2000/18806 (10.6%)]	Loss: 0.459567
Train epoch 3: [3000/18806 (16.0%)]	Loss: 0.396364
Train epoch 3: [4000/18806 (21.3%)]	Loss: 0.352623
Train epoch 3: [5000/18806 (26.6%)]	Loss: 0.311536
Train epoch 3: [6000/18806 (31.9%)]	Loss: 0.278969
Train epoch 3: [7000/18806 (37.2%)]	Loss: 0.250394
Train epoch 3: [8000/18806 (42.5%)]	Loss: 0.229850
Train epoch 3: [9000/18806 (47.9%)]	Loss: 0.221921
Train epoch 3: [10000/18806 (53.2%)]	Loss: 0.223900
Train epoch 3: [11000/18806 (58.5%)]	Loss: 0.251296
Train epoch 3: [12000/18806 (63.8%)]	Loss: 0.270486
Train epoch 3: [13000/18806 (69.1%)]	Loss: 0.292336
Train epoch 3: [14000/18806 (74.4%)]	Loss: 0.311252
Train epoch 3: [15000/18806 (79.8%)]	Loss: 0.327008
Train epoch 3: [16000/18806 (85.1%)]	Loss: 0.341771
Train epoch 3: [17000/18806 (90.4%)]	Loss: 0.334045
Train epoch 3: [18000/18806 (95.7%)]	Loss: 0.324998
====> Epoch 3: Average train loss 0.315036
====> Dev set loss: 0.052489
No improvement in 2 epochs (best dev set loss: 0.051630)
Not saving checkpoint; no improvement made
Train epoch 4: [1000/18806 (5.3%)]	Loss: 0.487376
Train epoch 4: [2000/18806 (10.6%)]	Loss: 0.423986
Train epoch 4: [3000/18806 (16.0%)]	Loss: 0.362052
Train epoch 4: [4000/18806 (21.3%)]	Loss: 0.323596
Train epoch 4: [5000/18806 (26.6%)]	Loss: 0.285582
Train epoch 4: [6000/18806 (31.9%)]	Loss: 0.255256
Train epoch 4: [7000/18806 (37.2%)]	Loss: 0.228564
Train epoch 4: [8000/18806 (42.5%)]	Loss: 0.210292
Train epoch 4: [9000/18806 (47.9%)]	Loss: 0.202904
Train epoch 4: [10000/18806 (53.2%)]	Loss: 0.203520
Train epoch 4: [11000/18806 (58.5%)]	Loss: 0.227447
Train epoch 4: [12000/18806 (63.8%)]	Loss: 0.243588
Train epoch 4: [13000/18806 (69.1%)]	Loss: 0.262800
Train epoch 4: [14000/18806 (74.4%)]	Loss: 0.279434
Train epoch 4: [15000/18806 (79.8%)]	Loss: 0.294107
Train epoch 4: [16000/18806 (85.1%)]	Loss: 0.307816
Train epoch 4: [17000/18806 (90.4%)]	Loss: 0.301446
Train epoch 4: [18000/18806 (95.7%)]	Loss: 0.292693
====> Epoch 4: Average train loss 0.283621
====> Dev set loss: 0.053023
No improvement in 3 epochs (best dev set loss: 0.051630)
STOPPING EARLY
Trained convolutional multidecoder.
