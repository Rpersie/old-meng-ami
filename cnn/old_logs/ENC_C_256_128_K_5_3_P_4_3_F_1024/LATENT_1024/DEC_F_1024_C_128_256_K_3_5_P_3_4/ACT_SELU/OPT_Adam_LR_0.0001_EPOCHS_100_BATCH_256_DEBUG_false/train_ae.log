Setting up environment...
Environment set up.
Training convolutional multidecoder...
Constructing model...
Done constructing model.
CNNMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 256, kernel_size=(5, 5), stride=(1, 1))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxpool2d_0): MaxPool2d (size=(1, 4), stride=(1, 4), dilation=(1, 1))
    (conv2d_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (3200 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 1024)
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 3200)
  )
  (decoder_deconv_ihm): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxunpool2d_1): MaxUnpool2d (size=(1, 4), stride=(1, 4), padding=(0, 0))
    (conv2d_1): Conv2d(256, 1, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4))
    (batchnorm2d_1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (SELU_0): SELU
    (lin_final): Linear (1024 -> 3200)
  )
  (decoder_deconv_sdm1): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxunpool2d_1): MaxUnpool2d (size=(1, 4), stride=(1, 4), padding=(0, 0))
    (conv2d_1): Conv2d(256, 1, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4))
    (batchnorm2d_1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 0.525657
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.446662
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.406825
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.368317
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.330342
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.300210
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.275710
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.258927
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.254510
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.266551
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.314500
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.349796
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.383215
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.410373
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.434340
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.457073
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.447349
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.435950
====> Epoch 1: Average train loss 0.424600
====> Dev set loss: 0.147854
New best dev set loss: 0.147854
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 0.569685
Train epoch 2: [2000/18806 (10.6%)]	Loss: 0.499446
Train epoch 2: [3000/18806 (16.0%)]	Loss: 0.435778
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.391859
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.348887
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.316258
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.287736
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.268921
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.263291
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.270092
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.309581
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.338050
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.366066
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.390913
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.410034
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.429854
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.421700
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.411519
====> Epoch 2: Average train loss 0.401055
====> Dev set loss: 0.155355
No improvement in 1 epochs (best dev set loss: 0.147854)
Not saving checkpoint; no improvement made
Train epoch 3: [1000/18806 (5.3%)]	Loss: 0.569008
Train epoch 3: [2000/18806 (10.6%)]	Loss: 0.497123
Train epoch 3: [3000/18806 (16.0%)]	Loss: 0.434013
Train epoch 3: [4000/18806 (21.3%)]	Loss: 0.389380
Train epoch 3: [5000/18806 (26.6%)]	Loss: 0.346632
Train epoch 3: [6000/18806 (31.9%)]	Loss: 0.313478
Train epoch 3: [7000/18806 (37.2%)]	Loss: 0.285038
Train epoch 3: [8000/18806 (42.5%)]	Loss: 0.265799
Train epoch 3: [9000/18806 (47.9%)]	Loss: 0.259875
Train epoch 3: [10000/18806 (53.2%)]	Loss: 0.265162
Train epoch 3: [11000/18806 (58.5%)]	Loss: 0.298165
Train epoch 3: [12000/18806 (63.8%)]	Loss: 0.322570
Train epoch 3: [13000/18806 (69.1%)]	Loss: 0.347525
Train epoch 3: [14000/18806 (74.4%)]	Loss: 0.369211
Train epoch 3: [15000/18806 (79.8%)]	Loss: 0.387692
Train epoch 3: [16000/18806 (85.1%)]	Loss: 0.404963
Train epoch 3: [17000/18806 (90.4%)]	Loss: 0.397456
Train epoch 3: [18000/18806 (95.7%)]	Loss: 0.388115
====> Epoch 3: Average train loss 0.378334
====> Dev set loss: 0.153759
No improvement in 2 epochs (best dev set loss: 0.147854)
Not saving checkpoint; no improvement made
Train epoch 4: [1000/18806 (5.3%)]	Loss: 0.541956
Train epoch 4: [2000/18806 (10.6%)]	Loss: 0.472630
Train epoch 4: [3000/18806 (16.0%)]	Loss: 0.408181
Train epoch 4: [4000/18806 (21.3%)]	Loss: 0.367739
Train epoch 4: [5000/18806 (26.6%)]	Loss: 0.327413
Train epoch 4: [6000/18806 (31.9%)]	Loss: 0.295780
Train epoch 4: [7000/18806 (37.2%)]	Loss: 0.268655
Train epoch 4: [8000/18806 (42.5%)]	Loss: 0.251105
Train epoch 4: [9000/18806 (47.9%)]	Loss: 0.245612
Train epoch 4: [10000/18806 (53.2%)]	Loss: 0.249507
Train epoch 4: [11000/18806 (58.5%)]	Loss: 0.279177
Train epoch 4: [12000/18806 (63.8%)]	Loss: 0.300459
Train epoch 4: [13000/18806 (69.1%)]	Loss: 0.322687
Train epoch 4: [14000/18806 (74.4%)]	Loss: 0.342063
Train epoch 4: [15000/18806 (79.8%)]	Loss: 0.359628
Train epoch 4: [16000/18806 (85.1%)]	Loss: 0.375840
Train epoch 4: [17000/18806 (90.4%)]	Loss: 0.369620
Train epoch 4: [18000/18806 (95.7%)]	Loss: 0.360551
====> Epoch 4: Average train loss 0.351600
====> Dev set loss: 0.158008
No improvement in 3 epochs (best dev set loss: 0.147854)
STOPPING EARLY
Trained convolutional multidecoder.
