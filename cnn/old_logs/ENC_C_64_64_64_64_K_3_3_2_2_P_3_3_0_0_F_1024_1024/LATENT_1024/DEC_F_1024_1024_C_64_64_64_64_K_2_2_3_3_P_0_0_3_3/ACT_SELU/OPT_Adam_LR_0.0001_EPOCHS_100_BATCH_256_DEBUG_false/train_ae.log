Setting up environment...
Environment set up.
Training convolutional multidecoder...
Constructing model...
Done constructing model.
CNNMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxpool2d_0): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (conv2d_3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))
    (batchnorm2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (1920 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (lin_final): Linear (1024 -> 1024)
    (SELU_final): SELU
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (lin_final): Linear (1024 -> 1920)
    (SELU_final): SELU
  )
  (decoder_deconv_ihm): Sequential (
    (conv2d_0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (conv2d_1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (maxunpool2d_3): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (lin_final): Linear (1024 -> 1920)
    (SELU_final): SELU
  )
  (decoder_deconv_sdm1): Sequential (
    (conv2d_0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (conv2d_1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (maxunpool2d_3): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 0.572905
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.471479
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.414971
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.371982
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.332700
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.301387
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.275109
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.256647
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.249719
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.258932
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.304905
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.338136
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.371306
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.397841
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.421033
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.442814
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.432323
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.420246
====> Epoch 1: Average train loss 0.407825
====> Dev set loss: 0.075961
New best dev set loss: 0.075961
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 0.564402
Train epoch 2: [2000/18806 (10.6%)]	Loss: 0.495275
Train epoch 2: [3000/18806 (16.0%)]	Loss: 0.431462
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.387321
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.344349
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.310690
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.280833
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.260206
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.251823
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.255238
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.291242
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.316409
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.343298
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.366836
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.384643
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.402988
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.394332
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.383723
====> Epoch 2: Average train loss 0.372375
====> Dev set loss: 0.083702
No improvement in 1 epochs (best dev set loss: 0.075961)
Not saving checkpoint; no improvement made
Train epoch 3: [1000/18806 (5.3%)]	Loss: 0.558455
Train epoch 3: [2000/18806 (10.6%)]	Loss: 0.487413
Train epoch 3: [3000/18806 (16.0%)]	Loss: 0.423362
Train epoch 3: [4000/18806 (21.3%)]	Loss: 0.378609
Train epoch 3: [5000/18806 (26.6%)]	Loss: 0.336050
Train epoch 3: [6000/18806 (31.9%)]	Loss: 0.302384
Train epoch 3: [7000/18806 (37.2%)]	Loss: 0.272782
Train epoch 3: [8000/18806 (42.5%)]	Loss: 0.251780
Train epoch 3: [9000/18806 (47.9%)]	Loss: 0.243328
Train epoch 3: [10000/18806 (53.2%)]	Loss: 0.245100
Train epoch 3: [11000/18806 (58.5%)]	Loss: 0.274387
Train epoch 3: [12000/18806 (63.8%)]	Loss: 0.295369
Train epoch 3: [13000/18806 (69.1%)]	Loss: 0.318990
Train epoch 3: [14000/18806 (74.4%)]	Loss: 0.339254
Train epoch 3: [15000/18806 (79.8%)]	Loss: 0.356305
Train epoch 3: [16000/18806 (85.1%)]	Loss: 0.372164
Train epoch 3: [17000/18806 (90.4%)]	Loss: 0.364465
Train epoch 3: [18000/18806 (95.7%)]	Loss: 0.354856
====> Epoch 3: Average train loss 0.344427
====> Dev set loss: 0.083717
No improvement in 2 epochs (best dev set loss: 0.075961)
Not saving checkpoint; no improvement made
Train epoch 4: [1000/18806 (5.3%)]	Loss: 0.527228
Train epoch 4: [2000/18806 (10.6%)]	Loss: 0.460057
Train epoch 4: [3000/18806 (16.0%)]	Loss: 0.395642
Train epoch 4: [4000/18806 (21.3%)]	Loss: 0.354806
Train epoch 4: [5000/18806 (26.6%)]	Loss: 0.314769
Train epoch 4: [6000/18806 (31.9%)]	Loss: 0.282832
Train epoch 4: [7000/18806 (37.2%)]	Loss: 0.254806
Train epoch 4: [8000/18806 (42.5%)]	Loss: 0.235418
Train epoch 4: [9000/18806 (47.9%)]	Loss: 0.227080
Train epoch 4: [10000/18806 (53.2%)]	Loss: 0.227711
Train epoch 4: [11000/18806 (58.5%)]	Loss: 0.253537
Train epoch 4: [12000/18806 (63.8%)]	Loss: 0.271294
Train epoch 4: [13000/18806 (69.1%)]	Loss: 0.292133
Train epoch 4: [14000/18806 (74.4%)]	Loss: 0.309985
Train epoch 4: [15000/18806 (79.8%)]	Loss: 0.325987
Train epoch 4: [16000/18806 (85.1%)]	Loss: 0.340533
Train epoch 4: [17000/18806 (90.4%)]	Loss: 0.333924
Train epoch 4: [18000/18806 (95.7%)]	Loss: 0.324405
====> Epoch 4: Average train loss 0.314764
====> Dev set loss: 0.079124
No improvement in 3 epochs (best dev set loss: 0.075961)
STOPPING EARLY
Trained convolutional multidecoder.
