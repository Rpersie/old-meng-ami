Setting up environment...
Environment set up.
Training convolutional variational multidecoder...
Constructing model...
Done constructing model.
CNNVariationalMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (maxpool2d_0): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (conv2d_3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))
    (batchnorm2d_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (1920 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
  )
  (latent_mu): Sequential (
    (lin): Linear (1024 -> 1024)
    (SELU): SELU
  )
  (latent_logvar): Sequential (
    (lin): Linear (1024 -> 1024)
    (SELU): SELU
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (lin_final): Linear (1024 -> 1920)
    (SELU_final): SELU
  )
  (decoder_deconv_ihm): Sequential (
    (conv2d_0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (conv2d_1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (maxunpool2d_3): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 1024)
    (batchnorm1d_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (lin_1): Linear (1024 -> 1024)
    (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (lin_final): Linear (1024 -> 1920)
    (SELU_final): SELU
  )
  (decoder_deconv_sdm1): Sequential (
    (conv2d_0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_0): SELU
    (conv2d_1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))
    (batchnorm2d_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_1): SELU
    (maxunpool2d_2): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (SELU_2): SELU
    (maxunpool2d_3): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (SELU_3): SELU
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 0.806405
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.695498
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.648530
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.594602
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.547444
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.508012
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.477636
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.453942
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.441945
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.448802
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.494917
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.529638
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.560289
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.584553
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.606118
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.627002
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.615658
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.602818
====> Epoch 1: Average train loss 0.590253
====> Dev set loss: 0.248674
New best dev set loss: 0.248674
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 0.715611
Train epoch 2: [2000/18806 (10.6%)]	Loss: 0.644538
Train epoch 2: [3000/18806 (16.0%)]	Loss: 0.588138
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.542225
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.497073
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.461608
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.431734
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.410794
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.402278
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.407444
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.449578
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.480758
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.508857
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.533545
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.552573
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.572858
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.564622
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.554186
====> Epoch 2: Average train loss 0.542932
====> Dev set loss: 0.237340
New best dev set loss: 0.237340
Saved checkpoint for model
Train epoch 3: [1000/18806 (5.3%)]	Loss: 0.734521
Train epoch 3: [2000/18806 (10.6%)]	Loss: 0.657759
Train epoch 3: [3000/18806 (16.0%)]	Loss: 0.595952
Train epoch 3: [4000/18806 (21.3%)]	Loss: 0.547914
Train epoch 3: [5000/18806 (26.6%)]	Loss: 0.501622
Train epoch 3: [6000/18806 (31.9%)]	Loss: 0.464793
Train epoch 3: [7000/18806 (37.2%)]	Loss: 0.433848
Train epoch 3: [8000/18806 (42.5%)]	Loss: 0.411762
Train epoch 3: [9000/18806 (47.9%)]	Loss: 0.402882
Train epoch 3: [10000/18806 (53.2%)]	Loss: 0.406845
Train epoch 3: [11000/18806 (58.5%)]	Loss: 0.443913
Train epoch 3: [12000/18806 (63.8%)]	Loss: 0.472347
Train epoch 3: [13000/18806 (69.1%)]	Loss: 0.498642
Train epoch 3: [14000/18806 (74.4%)]	Loss: 0.521194
Train epoch 3: [15000/18806 (79.8%)]	Loss: 0.540454
Train epoch 3: [16000/18806 (85.1%)]	Loss: 0.558586
Train epoch 3: [17000/18806 (90.4%)]	Loss: 0.550433
Train epoch 3: [18000/18806 (95.7%)]	Loss: 0.540127
====> Epoch 3: Average train loss 0.529160
====> Dev set loss: 0.224656
New best dev set loss: 0.224656
Saved checkpoint for model
Train epoch 4: [1000/18806 (5.3%)]	Loss: 0.726465
Train epoch 4: [2000/18806 (10.6%)]	Loss: 0.652175
Train epoch 4: [3000/18806 (16.0%)]	Loss: 0.586199
Train epoch 4: [4000/18806 (21.3%)]	Loss: 0.539562
Train epoch 4: [5000/18806 (26.6%)]	Loss: 0.494069
Train epoch 4: [6000/18806 (31.9%)]	Loss: 0.457473
Train epoch 4: [7000/18806 (37.2%)]	Loss: 0.426452
Train epoch 4: [8000/18806 (42.5%)]	Loss: 0.405405
Train epoch 4: [9000/18806 (47.9%)]	Loss: 0.396007
Train epoch 4: [10000/18806 (53.2%)]	Loss: 0.398313
Train epoch 4: [11000/18806 (58.5%)]	Loss: 0.432863
Train epoch 4: [12000/18806 (63.8%)]	Loss: 0.458463
Train epoch 4: [13000/18806 (69.1%)]	Loss: 0.482745
Train epoch 4: [14000/18806 (74.4%)]	Loss: 0.503348
Train epoch 4: [15000/18806 (79.8%)]	Loss: 0.521916
Train epoch 4: [16000/18806 (85.1%)]	Loss: 0.538950
Train epoch 4: [17000/18806 (90.4%)]	Loss: 0.531640
Train epoch 4: [18000/18806 (95.7%)]	Loss: 0.521506
====> Epoch 4: Average train loss 0.511052
====> Dev set loss: 0.242728
No improvement in 1 epochs (best dev set loss: 0.224656)
Not saving checkpoint; no improvement made
Train epoch 5: [1000/18806 (5.3%)]	Loss: 0.703442
Train epoch 5: [2000/18806 (10.6%)]	Loss: 0.626179
Train epoch 5: [3000/18806 (16.0%)]	Loss: 0.563679
Train epoch 5: [4000/18806 (21.3%)]	Loss: 0.519104
Train epoch 5: [5000/18806 (26.6%)]	Loss: 0.475321
Train epoch 5: [6000/18806 (31.9%)]	Loss: 0.441622
Train epoch 5: [7000/18806 (37.2%)]	Loss: 0.411830
Train epoch 5: [8000/18806 (42.5%)]	Loss: 0.391385
Train epoch 5: [9000/18806 (47.9%)]	Loss: 0.382250
Train epoch 5: [10000/18806 (53.2%)]	Loss: 0.384575
Train epoch 5: [11000/18806 (58.5%)]	Loss: 0.416953
Train epoch 5: [12000/18806 (63.8%)]	Loss: 0.439724
Train epoch 5: [13000/18806 (69.1%)]	Loss: 0.462276
Train epoch 5: [14000/18806 (74.4%)]	Loss: 0.481294
Train epoch 5: [15000/18806 (79.8%)]	Loss: 0.497526
Train epoch 5: [16000/18806 (85.1%)]	Loss: 0.514711
Train epoch 5: [17000/18806 (90.4%)]	Loss: 0.507828
Train epoch 5: [18000/18806 (95.7%)]	Loss: 0.498704
====> Epoch 5: Average train loss 0.489046
====> Dev set loss: 0.241582
No improvement in 2 epochs (best dev set loss: 0.224656)
Not saving checkpoint; no improvement made
Train epoch 6: [1000/18806 (5.3%)]	Loss: 0.668707
Train epoch 6: [2000/18806 (10.6%)]	Loss: 0.597453
Train epoch 6: [3000/18806 (16.0%)]	Loss: 0.541283
Train epoch 6: [4000/18806 (21.3%)]	Loss: 0.496597
Train epoch 6: [5000/18806 (26.6%)]	Loss: 0.454690
Train epoch 6: [6000/18806 (31.9%)]	Loss: 0.421476
Train epoch 6: [7000/18806 (37.2%)]	Loss: 0.393672
Train epoch 6: [8000/18806 (42.5%)]	Loss: 0.374669
Train epoch 6: [9000/18806 (47.9%)]	Loss: 0.365618
Train epoch 6: [10000/18806 (53.2%)]	Loss: 0.367354
Train epoch 6: [11000/18806 (58.5%)]	Loss: 0.395258
Train epoch 6: [12000/18806 (63.8%)]	Loss: 0.417559
Train epoch 6: [13000/18806 (69.1%)]	Loss: 0.438116
Train epoch 6: [14000/18806 (74.4%)]	Loss: 0.455763
Train epoch 6: [15000/18806 (79.8%)]	Loss: 0.472264
Train epoch 6: [16000/18806 (85.1%)]	Loss: 0.487125
Train epoch 6: [17000/18806 (90.4%)]	Loss: 0.480702
Train epoch 6: [18000/18806 (95.7%)]	Loss: 0.471960
====> Epoch 6: Average train loss 0.462997
====> Dev set loss: 0.232184
No improvement in 3 epochs (best dev set loss: 0.224656)
STOPPING EARLY
Computing reconstruction loss...
Loaded checkpoint; best model ready now.
====> Training set reconstruction loss: 0.165024
====> Dev set reconstruction loss: 0.161150
Trained convolutional variational multidecoder.
