Setting up environment...
Environment set up.
Training convolutional multidecoder...
Constructing model...
Done constructing model.
CNNMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))
    (Tanh_0): Tanh ()
    (maxpool2d_0): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))
    (Tanh_1): Tanh ()
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (7168 -> 2048)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (Tanh_1): Tanh ()
    (lin_final): Linear (2048 -> 1024)
    (Tanh_final): Tanh ()
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 2048)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (Tanh_1): Tanh ()
    (lin_final): Linear (2048 -> 7168)
    (Tanh_final): Tanh ()
  )
  (decoder_deconv_ihm): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (Tanh_0): Tanh ()
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (Tanh_1): Tanh ()
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 2048)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (Tanh_1): Tanh ()
    (lin_final): Linear (2048 -> 7168)
    (Tanh_final): Tanh ()
  )
  (decoder_deconv_sdm1): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (Tanh_0): Tanh ()
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (Tanh_1): Tanh ()
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 0.977782
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.909963
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.912683
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.882637
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.855068
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.826535
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.808863
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.790608
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.781326
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.789262
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.839736
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.882054
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.907587
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.925244
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.942080
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.959333
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.953819
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.945041
====> Epoch 1: Average train loss 0.939189
====> Dev set loss: 0.955926
New best dev set loss: 0.955926
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 1.148623
Train epoch 2: [2000/18806 (10.6%)]	Loss: 1.063464
Train epoch 2: [3000/18806 (16.0%)]	Loss: 1.041620
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.995194
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.952184
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.912554
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.884608
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.858638
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.844319
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.843132
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.882879
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.916872
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.934380
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.948777
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.959652
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.973651
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.968290
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.959363
====> Epoch 2: Average train loss 0.953002
