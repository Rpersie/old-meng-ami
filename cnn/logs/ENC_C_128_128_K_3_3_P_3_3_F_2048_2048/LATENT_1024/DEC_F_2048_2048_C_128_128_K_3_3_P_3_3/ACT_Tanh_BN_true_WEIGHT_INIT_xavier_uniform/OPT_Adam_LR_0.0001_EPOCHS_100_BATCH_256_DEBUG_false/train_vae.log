Setting up environment...
Environment set up.
Training convolutional variational multidecoder...
Constructing model...
Done constructing model.
CNNVariationalMultidecoder (
  (encoder_conv): Sequential (
    (conv2d_0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (maxpool2d_0): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
    (conv2d_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))
    (batchnorm2d_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
    (maxpool2d_1): MaxPool2d (size=(1, 3), stride=(1, 3), dilation=(1, 1))
  )
  (encoder_fc): Sequential (
    (lin_0): Linear (7168 -> 2048)
    (batchnorm1d_0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (batchnorm1d_1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
  )
  (decoder_fc_ihm): Sequential (
    (lin_0): Linear (1024 -> 2048)
    (batchnorm1d_0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (batchnorm1d_1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
    (lin_final): Linear (2048 -> 7168)
    (Tanh_final): Tanh ()
  )
  (decoder_deconv_ihm): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
  )
  (decoder_fc_sdm1): Sequential (
    (lin_0): Linear (1024 -> 2048)
    (batchnorm1d_0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (lin_1): Linear (2048 -> 2048)
    (batchnorm1d_1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
    (lin_final): Linear (2048 -> 7168)
    (Tanh_final): Tanh ()
  )
  (decoder_deconv_sdm1): Sequential (
    (maxunpool2d_0): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_0): Tanh ()
    (maxunpool2d_1): MaxUnpool2d (size=(1, 3), stride=(1, 3), padding=(0, 0))
    (conv2d_1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))
    (batchnorm2d_1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)
    (Tanh_1): Tanh ()
  )
  (latent_mu): Sequential (
    (lin): Linear (2048 -> 1024)
    (Tanh): Tanh ()
  )
  (latent_logvar): Sequential (
    (lin): Linear (2048 -> 1024)
    (Tanh): Tanh ()
  )
)
Setting up data...
Setting up training datasets...
Using 2407138 training features (9403 batches) for class ihm
Using 2407138 training features (9403 batches) for class sdm1
Setting up dev datasets...
Using 249565 dev features (975 batches) for class ihm
Using 249565 dev features (975 batches) for class sdm1
Setting up minibatch shuffling for training...
18806 total batches: {'ihm': 9403, 'sdm1': 9403}
Done setting up data.
Starting training!
Train epoch 1: [1000/18806 (5.3%)]	Loss: 1.031833
Train epoch 1: [2000/18806 (10.6%)]	Loss: 0.921217
Train epoch 1: [3000/18806 (16.0%)]	Loss: 0.865883
Train epoch 1: [4000/18806 (21.3%)]	Loss: 0.841144
Train epoch 1: [5000/18806 (26.6%)]	Loss: 0.815743
Train epoch 1: [6000/18806 (31.9%)]	Loss: 0.803554
Train epoch 1: [7000/18806 (37.2%)]	Loss: 0.790266
Train epoch 1: [8000/18806 (42.5%)]	Loss: 0.779943
Train epoch 1: [9000/18806 (47.9%)]	Loss: 0.772003
Train epoch 1: [10000/18806 (53.2%)]	Loss: 0.763243
Train epoch 1: [11000/18806 (58.5%)]	Loss: 0.756465
Train epoch 1: [12000/18806 (63.8%)]	Loss: 0.748616
Train epoch 1: [13000/18806 (69.1%)]	Loss: 0.745012
Train epoch 1: [14000/18806 (74.4%)]	Loss: 0.739331
Train epoch 1: [15000/18806 (79.8%)]	Loss: 0.736892
Train epoch 1: [16000/18806 (85.1%)]	Loss: 0.733045
Train epoch 1: [17000/18806 (90.4%)]	Loss: 0.729343
Train epoch 1: [18000/18806 (95.7%)]	Loss: 0.724950
====> Epoch 1: Average train loss 0.721979
====> Dev set loss: 0.461989
New best dev set loss: 0.461989
Saved checkpoint for model
Train epoch 2: [1000/18806 (5.3%)]	Loss: 0.674594
Train epoch 2: [2000/18806 (10.6%)]	Loss: 0.665434
Train epoch 2: [3000/18806 (16.0%)]	Loss: 0.658624
Train epoch 2: [4000/18806 (21.3%)]	Loss: 0.664799
Train epoch 2: [5000/18806 (26.6%)]	Loss: 0.658693
Train epoch 2: [6000/18806 (31.9%)]	Loss: 0.663112
Train epoch 2: [7000/18806 (37.2%)]	Loss: 0.661671
Train epoch 2: [8000/18806 (42.5%)]	Loss: 0.660358
Train epoch 2: [9000/18806 (47.9%)]	Loss: 0.660635
Train epoch 2: [10000/18806 (53.2%)]	Loss: 0.657973
Train epoch 2: [11000/18806 (58.5%)]	Loss: 0.657224
Train epoch 2: [12000/18806 (63.8%)]	Loss: 0.654371
Train epoch 2: [13000/18806 (69.1%)]	Loss: 0.654140
Train epoch 2: [14000/18806 (74.4%)]	Loss: 0.652983
Train epoch 2: [15000/18806 (79.8%)]	Loss: 0.653836
Train epoch 2: [16000/18806 (85.1%)]	Loss: 0.652957
Train epoch 2: [17000/18806 (90.4%)]	Loss: 0.652116
Train epoch 2: [18000/18806 (95.7%)]	Loss: 0.650274
====> Epoch 2: Average train loss 0.649179
====> Dev set loss: 0.431703
New best dev set loss: 0.431703
Saved checkpoint for model
Train epoch 3: [1000/18806 (5.3%)]	Loss: 0.642364
Train epoch 3: [2000/18806 (10.6%)]	Loss: 0.635291
Train epoch 3: [3000/18806 (16.0%)]	Loss: 0.629603
Train epoch 3: [4000/18806 (21.3%)]	Loss: 0.635377
Train epoch 3: [5000/18806 (26.6%)]	Loss: 0.630225
Train epoch 3: [6000/18806 (31.9%)]	Loss: 0.633527
Train epoch 3: [7000/18806 (37.2%)]	Loss: 0.632789
Train epoch 3: [8000/18806 (42.5%)]	Loss: 0.631909
Train epoch 3: [9000/18806 (47.9%)]	Loss: 0.632192
Train epoch 3: [10000/18806 (53.2%)]	Loss: 0.630103
Train epoch 3: [11000/18806 (58.5%)]	Loss: 0.629702
Train epoch 3: [12000/18806 (63.8%)]	Loss: 0.626313
Train epoch 3: [13000/18806 (69.1%)]	Loss: 0.626245
Train epoch 3: [14000/18806 (74.4%)]	Loss: 0.625198
Train epoch 3: [15000/18806 (79.8%)]	Loss: 0.626884
Train epoch 3: [16000/18806 (85.1%)]	Loss: 0.625796
Train epoch 3: [17000/18806 (90.4%)]	Loss: 0.625064
Train epoch 3: [18000/18806 (95.7%)]	Loss: 0.623271
====> Epoch 3: Average train loss 0.622280
====> Dev set loss: 0.459702
No improvement in 1 epochs (best dev set loss: 0.431703)
Not saving checkpoint; no improvement made
Train epoch 4: [1000/18806 (5.3%)]	Loss: 0.616215
Train epoch 4: [2000/18806 (10.6%)]	Loss: 0.610898
Train epoch 4: [3000/18806 (16.0%)]	Loss: 0.603469
Train epoch 4: [4000/18806 (21.3%)]	Loss: 0.609943
Train epoch 4: [5000/18806 (26.6%)]	Loss: 0.605272
Train epoch 4: [6000/18806 (31.9%)]	Loss: 0.608963
Train epoch 4: [7000/18806 (37.2%)]	Loss: 0.607790
Train epoch 4: [8000/18806 (42.5%)]	Loss: 0.606704
Train epoch 4: [9000/18806 (47.9%)]	Loss: 0.607424
Train epoch 4: [10000/18806 (53.2%)]	Loss: 0.605087
Train epoch 4: [11000/18806 (58.5%)]	Loss: 0.604644
Train epoch 4: [12000/18806 (63.8%)]	Loss: 0.601873
Train epoch 4: [13000/18806 (69.1%)]	Loss: 0.601832
Train epoch 4: [14000/18806 (74.4%)]	Loss: 0.600512
Train epoch 4: [15000/18806 (79.8%)]	Loss: 0.601942
Train epoch 4: [16000/18806 (85.1%)]	Loss: 0.600732
Train epoch 4: [17000/18806 (90.4%)]	Loss: 0.599835
Train epoch 4: [18000/18806 (95.7%)]	Loss: 0.598440
====> Epoch 4: Average train loss 0.597561
====> Dev set loss: 0.508863
No improvement in 2 epochs (best dev set loss: 0.431703)
Not saving checkpoint; no improvement made
Train epoch 5: [1000/18806 (5.3%)]	Loss: 0.592442
Train epoch 5: [2000/18806 (10.6%)]	Loss: 0.586388
Train epoch 5: [3000/18806 (16.0%)]	Loss: 0.578801
Train epoch 5: [4000/18806 (21.3%)]	Loss: 0.582854
Train epoch 5: [5000/18806 (26.6%)]	Loss: 0.578938
Train epoch 5: [6000/18806 (31.9%)]	Loss: 0.584263
Train epoch 5: [7000/18806 (37.2%)]	Loss: 0.583430
Train epoch 5: [8000/18806 (42.5%)]	Loss: 0.583226
Train epoch 5: [9000/18806 (47.9%)]	Loss: 0.582927
Train epoch 5: [10000/18806 (53.2%)]	Loss: 0.580709
Train epoch 5: [11000/18806 (58.5%)]	Loss: 0.580145
Train epoch 5: [12000/18806 (63.8%)]	Loss: 0.577449
Train epoch 5: [13000/18806 (69.1%)]	Loss: 0.577349
Train epoch 5: [14000/18806 (74.4%)]	Loss: 0.575721
Train epoch 5: [15000/18806 (79.8%)]	Loss: 0.577079
Train epoch 5: [16000/18806 (85.1%)]	Loss: 0.575859
Train epoch 5: [17000/18806 (90.4%)]	Loss: 0.574982
Train epoch 5: [18000/18806 (95.7%)]	Loss: 0.573127
====> Epoch 5: Average train loss 0.572122
====> Dev set loss: 0.559910
No improvement in 3 epochs (best dev set loss: 0.431703)
STOPPING EARLY
Computing reconstruction loss...
Loaded checkpoint; best model ready now.
====> Training set reconstruction loss: 0.350364
====> Dev set reconstruction loss: 0.336717
Trained convolutional variational multidecoder.
